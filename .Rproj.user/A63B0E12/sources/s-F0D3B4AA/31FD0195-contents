---
title: "Estimate impact of SLS in MH"
output: 
  html_document:
    theme: cosmo
    toc: true
    toc_depth: 2
    toc_float: true
---

In this notebook, I use synthetic controls to estimate the impact of Maharashtra's same language subtitling program on foundational literacy.

Research by Kothari (summarized [here](https://www.huffpost.com/entry/what-caused-maharashtras-leap-in-reading_b_589d1277e4b0e172783a9a8f) in the Huffington Post) shows that ASER scores increased in Maharashtra around the time that MH introduced SLS into programming. ASER scores actually didn't increase for grade 8 students in Maharashtra over this period, but since I don't have data on grade 8 students going back in time I am ignoring that for now. 


```{r setup, include=FALSE, message=FALSE} 
library(tidyverse); library(Synth); library(readxl);library(gt)
data_path <- "C:/Users/dougj/Documents/Data/Education/ASER trends over time"
```

```{r clean data, message=FALSE, include=FALSE}
# Import the data, convert everything to numeric, and keep just the variables we'll need
aser <- read_excel(file.path(data_path, "aser_trends_manual.xlsx")) %>% 
  mutate_at(vars(-state_abbr, -State), as.numeric) %>% 
  select(year, State, state_abbr, std3_read = std3_read_std1_all, std5_read = std5_read_std2_all, std3_math  = std3_subtract_all, std5_math = std5_divis_all) %>%
  filter(year != 2018)

```

# Descriptive statistics
Look at first differences in reading values to get a sense of how large the increase in MH is compared to other states.  For the class 5 indicator, MH has the largest increase from 2014 to 2016 and for the class 3 indicator it has the 3rd largest increase. 

```{r}
# Create dataset of first differences
diffs <- aser %>% filter(year >= 2012) %>% group_by(State) %>% 
  mutate_at(vars(starts_with("std")), ~.x-lag(.x, order_by = year)) %>% 
  filter(year != 2012) %>% 
  ungroup()

diffs_long <- diffs %>% pivot_longer(starts_with("std"), names_to = "temp", values_to = "diff") %>% 
  mutate(class = str_sub(temp, 4,4), subject = str_extract(temp, "(math|read)"))


# plot the reading changes for 2016
ggplot(subset(diffs_long, year == 2016), aes(x = State, y = diff, fill=factor(ifelse(State=="maharashtra","Highlighted","Normal")))) + 
  scale_fill_manual(name = "area", values=c("red","grey50"), guide= FALSE) +
  geom_bar(stat = "identity")+
  facet_grid(subject ~ class) +
  coord_flip() +
  labs(y = "Change from 2014 to 2016", x =element_blank(), title = "Change in ASER score 2014-2016 by state, subject and grade")

# plot the reading changes for 2014
ggplot(subset(diffs_long, year == 2014), aes(x = State, y = diff, fill=factor(ifelse(State=="maharashtra","Highlighted","Normal")))) + 
  scale_fill_manual(name = "area", values=c("red","grey50"), guide= FALSE) +
  geom_bar(stat = "identity")+
  facet_grid(subject ~ class) +
  coord_flip() +
  labs(y = "Change from 2013 to 2014", x =element_blank(), title = "Change in ASER score 2013-2014 by state, subject and grade")


# Print MH's values
diffs %>% filter(State == "maharashtra") %>% gt()

# Get the rank of Maharashtra for one variable
diffs %>% mutate_at(vars(-State),~min_rank(desc(.x))) %>% 
  filter(State == "maharashtra") %>% gt()

```

# Basic synthetic controls
Perform basic synthetic controls to estimate the impact on grade 5 reading levels. For predictors, I use...

* Mean of grade 5 reading for the period 2006-2014. (It seems a bit weird to just use the average, but this is what Abadie et al do in their example case.)
* Mean of grade 3 reading for the period 2006-2014
* Grade 3 reading for 2014 (since we would imagine that would be highly predictive)
* 2016 grade 5 math -- again, it feels a little weird to use a post-outcome variable as a predictor but in footnote 4 Abadie points out that this is kosher
* NSDP per capita in 2014 from the RBI

```{r}
# temp: Change 2016 to 2015 to see if it fixes the balanced panel error
aser$year[aser$year ==2016] <- 2015

# list the states with any NAs for std3_read or std5_read
states_w_na <- unique(aser$state_abbr[is.na(aser$std3_read) | is.na(aser$std5_read)])

# drop these states from the dataset (they are all small and unlikely to be good matches anyway). I also drop 2006 data because for some reason that is giving me the unbalanced error
aser <- aser %>% filter(!(state_abbr %in% states_w_na)) %>% 
  filter(year != 2006) 

# Create index for state 
aser <- aser %>% group_by(state_abbr) %>% mutate(id = group_indices())
  

dataprep.out <- dataprep(foo = as.data.frame(aser),
         predictors = c("std3_read", "std5_read"),
         predictors.op = "mean",
         time.predictors.prior = 2007:2014,
         special.predictors = list(
           list("nsdp_2014", 2014, "mean"),
           list("std3_read", 2014, "mean"),
           list("std5_math", 2015, "mean")
         ),
         dependent = "std5_read",
         unit.variable = "id",
         unit.names.variable = "state_abbr",
         time.variable = "year",
         treatment.identifier = 13,
         controls.identifier = seq(1,max(aser$id),1)[-13],
         time.optimize.ssr = 2007:2014,
         time.plot = 2007:2015)

synth.out <- synth(data.prep.obj = dataprep.out)
```

Estimate the impact of SLS!!!

```{r}
gaps <- dataprep.out$Y1plot - (dataprep.out$Y0plot %*% synth.out$solution.w)
gaps
```

Look at the synth.tables
```{r}
synth.tables <- synth.tab(dataprep.res = dataprep.out, synth.res = synth.out)

synth.tables$tab.pred
synth.tables$tab.w
synth.tables$tab.v
```







